{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI language model developed by OpenAI, based on the GPT-4 architecture. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llms import *\n",
    "answer = gpt_4o().run(\"what llm are you?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the LLM's available thru AWS Bedroock with this API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claude_3_haiku', 'claude_3_sonnet', 'claude_3_opus', 'claude_35_sonnet', 'claude_1_instant', 'claude_2', 'llama2_70b', 'llama3_8b_instruct', 'llama3_70b_instruct', 'mistral_7b_instruct', 'mistral_large', 'mistral_small', 'mixtral_8x7b_instruct', 'cohere_command_14', 'cohere_command_light_14', 'j2_mid', 'j2_ultra', 'titan_lite_v1', 'titan_express_v1', 'titan_premier_v1']\n"
     ]
    }
   ],
   "source": [
    "from llms.aws import *\n",
    "print(aws.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock Claude Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an artificial intelligence created by Anthropic. I don't have a specific name or model designation - I'm simply referred to as an \"AI assistant\" or \"helpful AI\". I'm not sure of the exact details of the language model or architecture that was used to create me. My capabilities come from the training data and techniques used by the Anthropic team, but the specifics are not something I have direct knowledge of. I'm an AI assistant here to help, but the details of my inner workings are a bit of a mystery even to me. Let me know if there's anything else I can assist with!\n"
     ]
    }
   ],
   "source": [
    "answer = claude_3_haiku().run(\"what llm are you?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock Llama Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an LLaMA, a large language model trained by a team of researcher at Meta AI. My model is based on the LLaMA architecture, which is a type of transformer-based language model. I'm designed to generate human-like text responses to a wide range of questions and topics, and I can engage in conversation, answer questions, and even create text based on a prompt. I'm constantly learning and improving, so please bear with me if I make any mistakes!\n"
     ]
    }
   ],
   "source": [
    "answer = llama3_70b_instruct().run(\"what llm are you?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
